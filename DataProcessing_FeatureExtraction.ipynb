{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.externals import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "def title_cleaner(title):\n",
    "    title = re.sub('[^a-zA-Z]',' ', title)\n",
    "    title = title.lower()\n",
    "    title = nltk.word_tokenize(title) \n",
    "    eng_stopwords = set(stopwords.words(\"english\"))\n",
    "    title = [w for w in title if not w in eng_stopwords]\n",
    "    title = ' '.join([word for word in title])\n",
    "    return(title)\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''Treebank to wordnet POS tag'''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n' #basecase POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reading the file\n",
    "data = pd.read_csv('fake.csv')\n",
    "\n",
    "del data['uuid']\n",
    "data = data.dropna().reset_index()\n",
    "del data['index']\n",
    "del data['thread_title']\n",
    "del data['spam_score']\n",
    "del data['main_img_url']\n",
    "del data['published']\n",
    "del data['crawled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'junksci', 'state', 'conspiracy', 'bs', 'bias', 'fake', 'hate'}\n"
     ]
    }
   ],
   "source": [
    "types = set(data['type'].tolist())\n",
    "print(types)\n",
    "result_type = [1 if val in ['bs', 'fake'] else 0 for val in data['type']]\n",
    "del data['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 1000 title for title_clean_original\n",
      "Done with 2000 title for title_clean_original\n",
      "Done with 3000 title for title_clean_original\n",
      "Done with 4000 title for title_clean_original\n",
      "Done with 500 title for text_clean_original\n",
      "Done with 1000 title for text_clean_original\n",
      "Done with 1500 title for text_clean_original\n",
      "Done with 2000 title for text_clean_original\n",
      "Done with 2500 title for text_clean_original\n",
      "Done with 3000 title for text_clean_original\n",
      "Done with 3500 title for text_clean_original\n",
      "Done with 4000 title for text_clean_original\n",
      "Done with 4500 title for text_clean_original\n"
     ]
    }
   ],
   "source": [
    "num_title = len(data['title'])\n",
    "title_clean_original = []\n",
    "for i in range(0,num_title):\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "         # print progress\n",
    "        print(\"Done with %d title for title_clean_original\" %(i+1)) \n",
    "    title_clean_original.append(title_cleaner(data['title'][i]))\n",
    "    \n",
    "num_text = len(data['text'])\n",
    "text_clean_original = []\n",
    "for i in range(0,num_text):\n",
    "    if( (i+1)%500 == 0 ):\n",
    "         # print progress\n",
    "        print(\"Done with %d title for text_clean_original\" %(i+1)) \n",
    "    text_clean_original.append(title_cleaner(data['text'][i]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 title for title_clean_wnl\n",
      "Done with 1000 title for title_clean_wnl\n",
      "Done with 1500 title for title_clean_wnl\n",
      "Done with 2000 title for title_clean_wnl\n",
      "Done with 2500 title for title_clean_wnl\n",
      "Done with 3000 title for title_clean_wnl\n",
      "Done with 3500 title for title_clean_wnl\n",
      "Done with 4000 title for title_clean_wnl\n",
      "Done with 4500 title for title_clean_wnl\n",
      "Done with 500 title for text_clean_wnl\n",
      "Done with 1000 title for text_clean_wnl\n",
      "Done with 1500 title for text_clean_wnl\n",
      "Done with 2000 title for text_clean_wnl\n",
      "Done with 2500 title for text_clean_wnl\n",
      "Done with 3000 title for text_clean_wnl\n",
      "Done with 3500 title for text_clean_wnl\n",
      "Done with 4000 title for text_clean_wnl\n",
      "Done with 4500 title for text_clean_wnl\n"
     ]
    }
   ],
   "source": [
    "title_clean_wnl = []\n",
    "for i, val in enumerate(title_clean_original):\n",
    "    if( (i+1)%500 == 0 ):\n",
    "         # print progress\n",
    "        print(\"Done with %d title for title_clean_wnl\" %(i+1)) \n",
    "    l = pos_tag(val.split())\n",
    "    temp = ' '.join([wnl.lemmatize(w,pos=get_wordnet_pos(t)) for w,t in l])\n",
    "    title_clean_wnl.append(temp)\n",
    "    \n",
    "text_clean_wnl = []\n",
    "for i, val in enumerate(text_clean_original):\n",
    "    if( (i+1)%500 == 0 ):\n",
    "         # print progress\n",
    "        print(\"Done with %d title for text_clean_wnl\" %(i+1)) \n",
    "    l = pos_tag(val.split())\n",
    "    temp = ' '.join([wnl.lemmatize(w,pos=get_wordnet_pos(t)) for w,t in l])\n",
    "    text_clean_wnl.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ord_in_thread</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>site_url</th>\n",
       "      <th>country</th>\n",
       "      <th>domain_rank</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>participants_count</th>\n",
       "      <th>likes</th>\n",
       "      <th>comments</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>muslim bust stole million gov benefit</td>\n",
       "      <td>print pay back money plus interest entire fami...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25689.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>attorney general loretta lynch plead fifth</td>\n",
       "      <td>attorney general loretta lynch plead fifth bar...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25689.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>break weiner cooperate fbi hillary email inves...</td>\n",
       "      <td>red state fox news sunday report morning antho...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25689.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>pin drop speech father daughter kidnap kill is...</td>\n",
       "      <td>email kayla mueller prisoner torture isi chanc...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25689.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>fantastic trump point plan reform healthcare b...</td>\n",
       "      <td>email healthcare reform make america great sin...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25689.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ord_in_thread  author                                              title  \\\n",
       "0              0   104.0              muslim bust stole million gov benefit   \n",
       "1              0  1023.0         attorney general loretta lynch plead fifth   \n",
       "2              0   104.0  break weiner cooperate fbi hillary email inves...   \n",
       "3              0   310.0  pin drop speech father daughter kidnap kill is...   \n",
       "4              0   310.0  fantastic trump point plan reform healthcare b...   \n",
       "\n",
       "                                                text  language  site_url  \\\n",
       "0  print pay back money plus interest entire fami...       3.0       0.0   \n",
       "1  attorney general loretta lynch plead fifth bar...       3.0       0.0   \n",
       "2  red state fox news sunday report morning antho...       3.0       0.0   \n",
       "3  email kayla mueller prisoner torture isi chanc...       3.0       0.0   \n",
       "4  email healthcare reform make america great sin...       3.0       0.0   \n",
       "\n",
       "   country  domain_rank  replies_count  participants_count  likes  comments  \\\n",
       "0     10.0      25689.0              0                   1      0         0   \n",
       "1     10.0      25689.0              0                   1      0         0   \n",
       "2     10.0      25689.0              0                   1      0         0   \n",
       "3     10.0      25689.0              0                   0      0         0   \n",
       "4     10.0      25689.0              0                   0      0         0   \n",
       "\n",
       "   shares  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = text_clean_wnl\n",
    "data['title'] = title_clean_wnl\n",
    "train_data = data.interpolate()\n",
    "le = LabelEncoder()\n",
    "def encode_columns():\n",
    "    l = ['country','site_url','author','language']\n",
    "    for col in l:\n",
    "        le.fit(train_data[col])\n",
    "        train_data[col]=le.transform(train_data[col])\n",
    "        train_data[col] = train_data[col].astype(float)\n",
    "encode_columns()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create y_train data for spam and result\n",
    "y_train_type = pd.DataFrame(result_type)\n",
    "# write trained_data to csv for quick training later\n",
    "train_data.to_csv('train_data.csv')\n",
    "y_train_type.to_csv('y_train_type.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 max_features = 250) \n",
    "\n",
    "def featurize(vectorizer):\n",
    "    return DataFrameMapper([(['ord_in_thread'], None),\n",
    "                            ('author', None),\n",
    "                            ('title', vectorizer),\n",
    "                            ('text', vectorizer),\n",
    "                            ('language', None),\n",
    "                            ('site_url', None),                                       \n",
    "                            ('country', None),\n",
    "                            ('domain_rank', None),\n",
    "                            ('replies_count', None),\n",
    "                            ('participants_count', None),\n",
    "                            ('likes', None),\n",
    "                            ('comments', None),\n",
    "                            ('shares', None)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create 50/50  random test/train split (can change test size ratio) for cross validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_data, y_train_type, test_size=0.50, random_state=42)\n",
    "y_train = y_train[0].tolist()\n",
    "y_test = y_test[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95023394300297748"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "pipe = Pipeline([('featurize', featurize(vec)), ('forest', forest)])\n",
    "pipe.fit_transform(x_train, y_train)\n",
    "x_test_pred = pipe.predict(x_test)\n",
    "accuracy = metrics.accuracy_score(y_test,x_test_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(le, 'label_encoder.pkl')\n",
    "joblib.dump(pipe, 'classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87324542747766909"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model, datasets\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "logreg = logreg.fit(x_train, y_train)\n",
    "\n",
    "x_test_pred = logreg.predict(x_test)\n",
    "accuracy  = metrics.accuracy_score(y_test,x_test_pred)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
